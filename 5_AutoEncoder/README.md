# Autoencoder

In __Neural Net__'s tutorial we saw that the network tries to predict the correct label corresponding to the input data. We saw that for MNIST dataset (which is a dataset of handwritten digits) we tried to predict the correct digit in the image. This type of machine learning algorithm is called __supervised learning__, simply because we are using __labels__.

__Autoencoder__ is neural networks that tries to __reconstruct the input data__. Since in training an __Autoencoder__ there are no labels involved, we have an __unsupervised learning__ method. By encoding the input data to a new space (which we usually call ___latent space__) we will have a new representation of the data. Two general types of __Autoencoders__ exist depending on the dimensionality of the latent space:

1. __dim(latent space) > dim(input space)__: This type of __Autoencoder__ is famous as __sparse autoencoder__. By having a large number of hidden units, __autoencoder__ will learn a usefull sparse representation of the data.

2. __dim(latent space) < dim(input space)__: This type of __Autoencoder__ has applications in __Dimensionality reduction__, __denoising__ and __learning the distribution of the data__. In this way the new representation (latent space) contains more essential information of the data


 __Autoencoder__ also helps us to understand how the neural networks work. We can visualize what a node has been experted on. This will give us an intuitive about the way these networks perform.


In this tutorial we will implement:
1. __Denoising autoencoder__ (1_noiseRemoval.ipynb):
2. __Visualizing activation of nodes in hidden layer__ (2_visActivation.ipynb)

## References:
* [www.tensorflow.com](www.tensorflow.com)
* [Visualizing a Trained Autoencoder](http://ufldl.stanford.edu/wiki/index.php/Visualizing_a_Trained_Autoencoder)
